{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Session 10 - Finetuning\n",
        "## Adapting Models to Domain-Specific Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Objectives:\n",
        "1. Transfer learning basics\n",
        "2. Parameter-efficient finetuning\n",
        "3. LoRA and Adapters\n",
        "4. Evaluation metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Transfer Learning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Transfer Learning:')\nprint('1. Pre-training: Large model on big data')\nprint('2. Finetuning: Adapt to target domain')\nprint()\nprint('Benefits:')\nprint('\u2713 Faster training')\nprint('\u2713 Better performance')\nprint('\u2713 Lower cost')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Parameter-Efficient Finetuning (PEFT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "methods = {\n    'Full Finetune': '100%',\n    'LoRA': '0.1-1%',\n    'Adapters': '1-10%',\n    'BitFit': '0.01%'\n}\nfor method, params in methods.items():\n    print(f'{method}: {params} parameters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LoRA (Low-Rank Adaptation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n\nlora_config = LoraConfig(\n    r=8,\n    lora_alpha=32,\n    target_modules=['q_proj', 'v_proj'],\n    lora_dropout=0.05\n)\nprint('\u2713 LoRA config ready')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Finetuning Workflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM\n\nmodel = AutoModelForCausalLM.from_pretrained('gpt2')\nprint(f'Model: gpt2')\nprint(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n    'learning_rate': 5e-4,\n    'batch_size': 8,\n    'epochs': 3,\n    'warmup_steps': 100\n}\nfor k, v in config.items():\n    print(f'{k}: {v}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Evaluation Metrics\n\n| Metric | Range | Purpose |\n|--------|-------|----------|\n| Perplexity | 1+ | LM quality |\n| BLEU | 0-100 | Translation |\n| Accuracy | 0-100% | Classification |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Best Practices\n\n\u2713 Use LoRA for efficiency\n\u2713 Higher learning rate\n\u2713 Monitor loss curves\n\u2713 Validate regularly\n\u2713 Save best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusions\n\n\u2713 Transfer learning is powerful\n\u2713 LoRA reduces parameters 99%\n\u2713 Finetuning beats retraining"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}